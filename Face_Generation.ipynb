{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face Generation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oBfv-9uLyKj",
        "colab_type": "text"
      },
      "source": [
        "**IMPORTANT LINKS**\n",
        "\n",
        "[Descriptive article](https://towardsdatascience.com/fake-face-generator-using-dcgan-model-ae9322ccfd65)\n",
        "\n",
        "[General overview article](https://www.lyrn.ai/2018/12/26/a-style-based-generator-architecture-for-generative-adversarial-networks/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8b9GETDQJHE",
        "colab_type": "text"
      },
      "source": [
        "**PLASE NOTE THAT THIS CODE WILL FAIL TO RUN SUCCESFULLY ON THE CLOUD, AND MUST BE RUN ON A LOCAL MACHINE**\n",
        "\n",
        "This is currently simply here just to be viewed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCjtuj6RRCc6",
        "colab_type": "text"
      },
      "source": [
        "**NOTE**\n",
        "\n",
        "CODE IS IN FINAL STATE\n",
        "NOT TO FURTHER BE EDITED EXCEPT MODIFICATIONS FOR FINAL SUBMISSION\n",
        "\n",
        "NICE TEAMWORK!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "leDzZEBiQwaK",
        "colab": {}
      },
      "source": [
        "# IMPORTS\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "from glob import glob\n",
        "import datetime\n",
        "import random\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEiORhJBqQ9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(z, outputChannelDim, training):\n",
        "\twith tf.variable_scope(\"generator\", reuse= not training):\n",
        "\t\t# 8 by 8 by 1024\n",
        "\t\tfullyConnected = tf.layers.dense(z, 8*8*1024)\n",
        "\t\tfullyConnected = tf.reshape(fullyConnected, (-1, 8, 8, 1024))\n",
        "\t\tfullyConnected = tf.nn.leaky_relu(fullyConnected)\n",
        "\n",
        "\t\t# 16 by 16 by 512\n",
        "\t\ttransConv1 = tf.layers.conv2d_transpose(inputs = fullyConnected, filters=512, kernel_size=[5,5], strides=[2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name = \"transConv1\")\n",
        "\t\tbatchTransConv1 = tf.layers.batch_normalization(inputs = transConv1, training=training, epsilon=EPSILON, name=\"batchTransConv1\")\n",
        "\t\ttransConv1Out = tf.nn.leaky_relu(batchTransConv1, name=\"transConv1Out\")\n",
        "\n",
        "\t\t# 32 by 32 by 256\n",
        "\t\ttransConv2 = tf.layers.conv2d_transpose(inputs=transConv1Out, filters=256, kernel_size=[5,5], strides=[2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"transConv2\")\n",
        "\t\tbatchTransConv2 = tf.layers.batch_normalization(inputs=transConv2, training=training, epsilon=EPSILON, name=\"batchTransConv2\")\n",
        "\t\ttransConv2Out = tf.nn.leaky_relu(batchTransConv2, name=\"transConv2Out\")\n",
        "\n",
        "\t\t# 64 by 64 by 128\n",
        "\t\ttransConv3 = tf.layers.conv2d_transpose(inputs=transConv2Out, filters=128, kernel_size=[5,5], strides=[2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"transConv3\")\n",
        "\t\tbatchTransConv3 = tf.layers.batch_normalization(inputs=transConv3, training=training, epsilon=EPSILON, name=\"batchTransConv3\")\n",
        "\t\ttransConv3Out = tf.nn.leaky_relu(batchTransConv3, name=\"transConv3Out\")\n",
        "\n",
        "\t\t# 128 by 128 by 64\n",
        "\t\ttransConv4 = tf.layers.conv2d_transpose(inputs=transConv3Out, filters=64, kernel_size=[5,5], strides=[2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"transConv4\")\n",
        "\t\tbatchTransConv4 = tf.layers.batch_normalization(inputs=transConv4, training=training, epsilon=EPSILON, name=\"batchTransConv4\")\n",
        "\t\ttransConv4Out = tf.nn.leaky_relu(batchTransConv4, name=\"transConv4Out\")\n",
        "\n",
        "\t\t# 128 by 128 by 3\n",
        "\t\tlogits = tf.layers.conv2d_transpose(inputs=transConv4Out, filters=3, kernel_size=[5,5], strides=[1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"logits\")\n",
        "\t\toutput = tf.tanh(logits, name=\"output\")\n",
        "\n",
        "\t\treturn output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMZ72HOjuoG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator(x, reuse):\n",
        "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
        "        # 64 by 64 by 64\n",
        "        conv1 = tf.layers.conv2d(inputs=x, filters=64, kernel_size=[5,5], strides=[2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"conv1\")\n",
        "        batchNorm1 = tf.layers.batch_normalization(conv1, training=True, epsilon=EPSILON, name=\"batchNorm1\")\n",
        "        conv1Out = tf.nn.leaky_relu(batchNorm1, name=\"conv1Out\")\n",
        "\n",
        "        # 32 by 32 by 128\n",
        "        conv2 = tf.layers.conv2d(inputs=conv1Out, filters=128, kernel_size=[5,5], strides=[2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"conv2\")\n",
        "        batchNorm2 = tf.layers.batch_normalization(conv2, training=True, epsilon=EPSILON, name=\"batchConv2\")\n",
        "        conv2Out = tf.nn.leaky_relu(batchNorm2, name=\"conv2Out\")\n",
        "\n",
        "        # 16 by 16 by 256\n",
        "        conv3 = tf.layers.conv2d(inputs=conv2Out, filters=256, kernel_size=[5,5], strides=[2,2], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"conv3\")\n",
        "        batchNorm3 = tf.layers.batch_normalization(conv3, training=True, epsilon=EPSILON, name=\"batchNorm3\")\n",
        "        conv3Out = tf.nn.leaky_relu(batchNorm3, name=\"conv3Out\")\n",
        "\n",
        "        # 16 by 16 by 512\n",
        "        conv4 = tf.layers.conv2d(inputs=conv3Out, filters=512, kernel_size=[5,5], strides=[1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"conv4\")\n",
        "        batchNorm4 = tf.layers.batch_normalization(conv4, training=True, epsilon=EPSILON, name=\"batchNorm4\")\n",
        "        conv4Out = tf.nn.leaky_relu(batchNorm4, name=\"conv4Out\")\n",
        "\n",
        "        # 8 by 8 by 1024\n",
        "        conv5 = tf.layers.conv2d(inputs=conv4Out, filters=1024, kernel_size=[5,5], strides=[1,1], padding=\"SAME\", kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV), name=\"conv5\")\n",
        "        batchNorm5 = tf.layers.batch_normalization(conv5, training=True, epsilon=EPSILON, name=\"batchNorm5\")\n",
        "        conv5Out = tf.nn.leaky_relu(batchNorm4, name=\"conv5Out\")\n",
        "\n",
        "        flatten = tf.reshape(conv5Out, (-1, 8*8*1024))\n",
        "        logits = tf.layers.dense(inputs=flatten, units=1, activation=None)\n",
        "        output = tf.sigmoid(logits)\n",
        "        return output, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmS4mlUFyLsx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelLoss(inputReal, inputZ, outputChannelDim):\n",
        "    gModel = generator(inputZ, outputChannelDim, True)\n",
        "\n",
        "    noisyInputReal = inputReal + tf.random_normal(shape=tf.shape(inputReal), mean=0.0, stddev=random.uniform(0.0, 0.1), dtype=tf.float32)\n",
        "\n",
        "    dModelReal, dLogitsReal = discriminator(noisyInputReal, reuse=False)\n",
        "    dModelFake, dLogitsFake = discriminator(gModel, reuse=True)\n",
        "\n",
        "    dLossReal = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dLogitsReal, labels=tf.ones_like(dModelReal)*random.uniform(0.9, 1.0)))\n",
        "    dLossFake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dLogitsFake, labels=tf.zeros_like(dModelFake)))\n",
        "\n",
        "    dLoss = tf.reduce_mean(0.5 * (dLossReal + dLossFake))\n",
        "    gLoss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=dLogitsFake, labels=tf.ones_like(dModelFake)))\n",
        "    \n",
        "    return dLoss, gLoss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxNsb9XOzJXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelOptimizers(dLoss, gLoss):\n",
        "    tVars = tf.trainable_variables()\n",
        "    gVars = [var for var in tVars if var.name.startswith(\"generator\")]\n",
        "    dVars = [var for var in tVars if var.name.startswith(\"discriminator\")]\n",
        "\n",
        "    updateOps = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    genUpdates = [op for op in update_ops if op.name.startswith(\"generator\") or op.name.startswith(\"discriminator\")]\n",
        "\n",
        "    with tf.control_dependencies(genUpdates):\n",
        "        dTrainOpt = tf.train.AdamOptimizer(learning_rate=LEARN_D, beta1=BETA1).minimize(dLoss, var_list=dVars)\n",
        "        gTrainOpt = tf.train.AdamOptimizer(learning_rate=LEARN_G, beta1=BETA1).minimize(gLoss, var_list=gVars)\n",
        "        \n",
        "    return dTrainOpt, gTrainOpt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WeZYSdvz5w3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def modelInputs(realDim, zDim):\n",
        "    inputsReal = tf.placeholder(tf.float32, (None, *realDim), name=\"inputsReal\")\n",
        "    inputsZ = tf.placeholder(tf.float32, (None, zDim), name=\"inputZ\")\n",
        "    learningRateG = tf.placeholder(tf.float32, name=\"learningRateG\")\n",
        "    learningRateD = tf.placeholder(tf.float32, name=\"learningRateD\")\n",
        "    return inputsReal, inputsZ, learningRateG, learningRateD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxl6dWWB0VDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function deprecated, does not work on local machines; therefore has been omitted\n",
        "# FUNCTION TO BE ALTERED TO ENABLE IMAGE SAVING ON LOCAL PC\n",
        "\"\"\"def showSamples(sampleImages, name, epoch):\n",
        "    figure, axes = plt.subplots(1, len(sampleImages), figsize=(IMG_SIZE, IMG_SIZE))\n",
        "    for index, axis in enumerate(axes):\n",
        "        axis.axis('off')\n",
        "        imageArray = sampleImages[index]\n",
        "        axis.imshow(imageArray)\n",
        "        image = Image.fromarray(imageArray)\n",
        "        image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\")\n",
        "    plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
        "    plt.show()\n",
        "    plt.close() \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Vp8zOL017J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(sess, inputZ, outChannelDim, epoch):\n",
        "    exampleZ = np.random.uniform(-1, 1, size=[SAMPLES_TO_SHOW, inputZ.get_shape().as_list()[-1]])\n",
        "    samples=sess.run(generator(inputZ, outChannelDim, False), feed_dict={inputZ: exampleZ})\n",
        "    sampleImages = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n",
        "    #show_samples(sampleImages, OUTPUT_DIR + \"samples\", epoch) -- THIS LINE TO BE OMITTED"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHr7yKpl1bia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summEpoch(epoch, sess, dLosses, gLosses, inputZ, dataShape, saver):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch, EPOCHS), \"\\nD Loss: {:.5f}\".format(np.mean(dLosses[-MINIBATCH_SIZE:])), \"\\nG Loss: {:.5f}\".format(np.mean(gLosses[-MINIBATCH_SIZE:])))\n",
        "    # FOLLOWING LINES TO BE OMITTED\n",
        "    \"\"\"\n",
        "\tfig, ax = plt.subplots()\n",
        "    plt.plot(dLosses, label='Discriminator', alph=0.6)\n",
        "    plt.plot(gLosses, label='Generator', alpha=0.6)\n",
        "    plt.title(\"Losses\")\n",
        "    plt.legend()\n",
        "    plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\t\"\"\"\n",
        "    saver.save(sess, OUTPUT_DIR + \"model_\" + str(epoch) + \".ckpt\")\n",
        "    test(sess, inputZ, dataShape[3], epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQOsGThCQ0Pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getBatch(dataset):\n",
        "\tfiles = random.sample(dataset, BATCH_SIZE)\n",
        "\tbatch = []\n",
        "\tfor file in files:\n",
        "\t\tif random.choice([True, False]):\n",
        "\t\t\tbatch.append(np.asarray(Image.open(file).transpose(Image.FLIP_LEFT_RIGHT)))\n",
        "\t\telse:\n",
        "\t\t\tbatch.append(np.asarray(Image.open(file)))\n",
        "\tbatch = np.asarray(batch)\n",
        "\tnormalizedBatch = (batch / 127.5) - 1.0\n",
        "\treturn normalizedBatch, files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJxfpz122gHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataShape, epoch, checkpointPath):\n",
        "    inputImages, inputZ, learningRateG, learningRateD = modelInputs(dataShape[1:], NOISE_SIZE)\n",
        "    dLoss, gLoss = modelLoss(inputImages, inputZ, dataShape[3])\n",
        "    dOpt, gOpt = modelOptimizers(dLoss, gLoss)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        if checkpointPath is not None:\n",
        "            saver.restore(sess, checkpointPath)\n",
        "\n",
        "        iteration = 0\n",
        "        dLosses = []\n",
        "        gLosses = []\n",
        "\n",
        "        for epoch in range(EPOCH, EPOCHS):\n",
        "            epoch += 1\n",
        "            epochDataset = DATASET.copy()\n",
        "\n",
        "            for i in range(MINIBATCH_SIZE):\n",
        "                iterationStartTime = time.time()\n",
        "                iteration += 1\n",
        "                batchImages, usedFiles = getBatch(epochDataset)\n",
        "                [epochDataset.remove(file) for file in usedFiles]\n",
        "\n",
        "                batchZ = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_SIZE))\n",
        "                _ = sess.run(dOpt, feed_dict={inputImages: batchImages, inputZ: batchZ, learningRateD: LEARN_D})\n",
        "                _ = sess.run(gOpt, feed_dict={inputImages: batchImages, inputZ: batchZ, learningRateG: LEARN_G})\n",
        "                dLosses.append(dLoss.eval({inputZ: batchZ, inputImages: batchImages}))\n",
        "                gLosses.append(gLoss.eval({inputZ: batchZ}))\n",
        "\n",
        "                elapsedTime = round(time.time()-iterationStartTime, 3)\n",
        "                remainingFiles = len(epochDataset)\n",
        "                print(\"\\rEpoch: \" + str(epoch) +\n",
        "                      \", iteration: \" + str(iteration) + \n",
        "                      \", d_loss: \" + str(round(dLosses[-1], 3)) +\n",
        "                      \", g_loss: \" + str(round(gLosses[-1], 3)) +\n",
        "                      \", duration: \" + str(elapsedTime) + \n",
        "                      \", minutes remaining: \" + str(round(remainingFiles/BATCH_SIZE*elapsedTime/60, 1)) +\n",
        "                      \", remaining files in batch: \" + str(remainingFiles))\n",
        "                summEpoch(epoch, sess, dLosses, gLosses, inputZ, dataShape, saver)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RosDWRyG5CLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HYPERPARAMS\n",
        "IMAGE_SIZE = 128\n",
        "NOISE_SIZE = 100\n",
        "LEARN_D = 0.00004\n",
        "LEARN_G = 0.0002\n",
        "BATCH_SIZE = 64\n",
        "EPOCH = 0 # not 0 if resuming session\n",
        "EPOCHS = 8\n",
        "BETA1 = 0.5\n",
        "WEIGHT_INIT_STDDEV = 0.02\n",
        "EPSILON = 0.00005\n",
        "SAMPLES_TO_SHOW = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jrVYR305X4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_PATH=\"../input/\"\n",
        "DATASET_LIST_PATH = BASE_PATH + \"100k.txt\"\n",
        "INPUT_DATA_DIR = BASE_PATH + \"100k/100k/\"\n",
        "OUTPUT_DIR = \"./\"\n",
        "DATASET = [INPUT_DATA_DIR + str(line).rstrip() for line in open(DATASET_LIST_PATH, \"r\")]\n",
        "DATASET_SIZE = len(DATASET)\n",
        "MINIBATCH_SIZE = DATASET_SIZE // BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kesi1fsoQ9qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to resume training:\n",
        "# MODEL_PATH = BASE_PATH + \"models/\" + \"model_\" + str(EPOCH) + \".ckpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNI9WlH465yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Execution!\n",
        "with tf.Graph().as_default():\n",
        "    train(dataShape=(DATASET_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3), epoch=EPOCH, checkpointPath=None)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}